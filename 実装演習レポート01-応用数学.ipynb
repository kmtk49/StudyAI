{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18182cc6",
   "metadata": {},
   "source": [
    "# 応用数学 基準点3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad02087",
   "metadata": {},
   "source": [
    "## 第一章　線形代数 \n",
    "要点まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c841cf",
   "metadata": {},
   "source": [
    "\n",
    "行列とはスカラーを表にしたもので、連立方程式など複数の関数・数式を表現することが可能。（行基本変形）<br>\n",
    "表現する際、行列の積として表すが、その性質上掛ける側の行列の行数と掛けられる側の列数が同じでないといけない。<br>\n",
    "\n",
    "機械学習における下記項目の意義・必要性をまとめる。またそれぞれの意味を簡潔に\n",
    "- 単位行列 \n",
    "$$\n",
    "\\left(\\begin{array}{c}\n",
    "1&&0\\\\\n",
    "& \\ddots\\\\\n",
    "0&& 1\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "性質\n",
    "$$\n",
    "IA = AI = A\n",
    "$$\n",
    "- 対角行列\n",
    "$$\n",
    "D = \\\n",
    "\\left(\\begin{array}{c}\n",
    "x_1&&0\\\\\n",
    "& \\ddots\\\\\n",
    "0&& x_n\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "対称行列の特徴：対応する固有ベクトルを列として並べた行列P、固有値λを対角成分に並べた行列Dに対し\n",
    "$$\n",
    "A = PDP^-1\n",
    "$$\n",
    "が成立する。これを「固有値分解」という\n",
    "- 対称行列\n",
    "$$\n",
    "A = \\\n",
    "\\left(\\begin{array}{c}\n",
    "x_1&\\\\\n",
    "& \\ddots\\\\\n",
    "&& x_n\\\\\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "- 逆行列　逆数のような働き \n",
    "$$  \n",
    "AA^{-1} = I\n",
    "$$\n",
    "単位行列・逆行列・行列の積を活用することで様々な行列計算が可能になる。<br>掃き出し法によって逆行列を求められる<br>\n",
    "ただし逆行列が存在しない場合がある。<br>\n",
    "\n",
    "-　単射でない。（解が一つに定まらない。ただし範囲を設定し、単射にすればOK）\n",
    " \n",
    "$$\n",
    "\\left(\\begin{array}{}\n",
    "a&b\\\\\n",
    "c&d\\\\\n",
    "\\end{array}\\right)\\  ad =bd\n",
    "$$\n",
    "逆行列が存在しない　= 平行四辺形の面積がゼロになる\n",
    "-逆行列が存在するかどうかの判別式\n",
    "#### 行列式\n",
    "$$\n",
    "\\left|\\begin{array}\\\n",
    "a&b\\\\\n",
    "c&d\\\\\n",
    "\\end{array}\\right|\\ = \n",
    "\\left|\\begin{array}\\\n",
    "\\vec{v_1}\\\\\n",
    "\\vec{v_2}\n",
    "\\end{array}\\right|\\\\\n",
    "||は絶対値と同義\n",
    "$$\n",
    "-　行列式の成分が重複している（途中まで解けても最後までは解けない）<br>\n",
    "-　一つの成分がX倍されるとすべてX倍（行列の特徴・ベクトルで挟まれた空間の面積として考える）\n",
    "<img src =\"https://assets.st-note.com/production/uploads/images/26665513/rectangle_large_type_2_465d94d9affc430d9c6b62eb3c384964.png?width=500\" width=\"500\">\n",
    "\n",
    "このとき，元の正方形の面積は 1，変換後の平行四辺形の面積は |A|．つまり，行列式 |A| は，線形変換 A によって，正方形の面積が何倍になるかを意味している．\n",
    "<img src =\"https://assets.st-note.com/production/uploads/images/26668716/picture_pc_9f8050156735ec3ce3316a6d34b0fa48.png?width=500\" widgh=\"100\">\n",
    "\n",
    "### 逆行列の求め方　公式　2次正方行列のとき\n",
    "<img src =\"https://assets.st-note.com/production/uploads/images/26665875/picture_pc_6eccf09912e51aa9b3b04d6ed4c0929f.png?width=800\" width=\"500\">\n",
    "ref.「行列式|A|=ad-bcの幾何学的意味」\n",
    "https://note.com/dr_kano/n/n94dfeb646cad\n",
    "\n",
    "### 機械学習における行列式の意義\n",
    "行列式が2つのベクトル(a,b)における面積であれば、対角線cはa-bに直行する。これはベクトルy(予測値)、t（教師データ）、に対する対角線w（ウエイト）ととらえることができる。\n",
    "$$\n",
    "\\cos\\theta = \\frac{\\vec{y}\\vec{t}}{|\\vec{y}||\\vec{t}|}\n",
    "$$\n",
    "-　固有値・固有ベクトル\n",
    "行列Aの対して以下の式が成り立つとき、ベクトルⅹを固有ベクトル、係数λを固有値という。\n",
    "$$A\\vec{x} =\\lambda\\vec{x} $$\n",
    "-　特異値分解（固有値分解との違いも）\n",
    "ref. 「特異値分解を詳しく解説」https://qiita.com/kidaufo/items/0f3da4ca4e19dc0e987e\n",
    "<br>-　予備知識　ランクについて\n",
    " 2次正方行列Aを写像と考えた場合、原点を中心とした正方形をA座標分歪ませた四角形ととらえることができる。\n",
    " これを元の正方形に戻す写像がAの逆行列。ということになる。\n",
    " $$ PAx = x <=> PA = I <=> P = A^{-1}$$\n",
    "この際、n次行列のうち一時独立な列（または行）がnでないと写像を戻すことができない。つまり逆行列が存在しない。\n",
    "このランクといい、rank(A) = 2と表示する。\n",
    "\n",
    "$$ A = UrV^T$$\n",
    "\n",
    "<img src ='https://camo.qiitausercontent.com/a4fe860686a4ca8b95d6105ec332f3749242e743/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e616d617a6f6e6177732e636f6d2f302f3133323230382f30633062353238302d613738612d653764612d633064332d3162366632663465353765372e706e67' width='500'>\n",
    "\n",
    "### 機械学習における特異値分解の意義\n",
    "特異値分解を用いることで行列の情報を維持したまま次元を減らすことが可能になる。この仕組みは次元削減の理論や画像変換等に利用される。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f14861",
   "metadata": {},
   "source": [
    "## 補足　基礎解析\n",
    "-　関数の最適化によって最適解を求めること。この際微分を用いることが多い。\n",
    "-　複雑な関数を扱う場合、微分係数を計算することで関数のトレンドを知ることができる\n",
    "-　関数の最適化は、最小値や極小値を求めること→目的関数の微分\n",
    "\n",
    "### 偏微分\n",
    "多変数関数の微分\n",
    "主にニューラルネットワークのバックプロパゲーションの際に活用する。\n",
    "（活性uをそれぞれのインプットノードに振り分けて微分する等）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c562e7",
   "metadata": {},
   "source": [
    "## 第二章　確率・統計\n",
    "要点まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d327bf7",
   "metadata": {},
   "source": [
    "手元のデータから、未来の不確実さを低減させる手法\n",
    "誤差を正規分布で表現できると、ノイズを数学的に扱える。→データ解析に用いることができる。\n",
    "\n",
    "- 離散的データ　→　確率質量関数\n",
    "- 連続データ　　→　確率密度関数\n",
    "\n",
    "### ベイズ確率\n",
    "$$\n",
    "P(A|B) = \\frac{P(A\\cap B)}{P(B)}　= \\frac{P(A)P(B|A)乗法定理}{P(B)}\n",
    "$$\n",
    "\n",
    "### 期待値\n",
    "元々は想定外の理由で賭け事が中止になった際、それぞれの状態に応じて報酬を分配するために考案\n",
    "確率変数f(x) x 確率P(x) の総和　=$$\n",
    "期待値E(f) = \n",
    "\\sum_{k=1}^{n}P(X = x_k)f(X =x_k)$$\n",
    "\n",
    "\n",
    "## 機械学習における下記項目の意義・必要性\n",
    "-　ベイズ確率\n",
    "-　分散と共分散\n",
    "-　分散と標準偏差\n",
    "-　ベルヌーイ分布・マルチヌーイ分布\n",
    "コイントスのように2つの結果が約束されている確率の分布。マルチヌーイ分布は多項分布とも呼び、ベルヌーイ分布を一般化（サイコロのように2個以上の結果が約束されている確率の分布）。それぞれ確率の分布は等しくなくてもよい。\n",
    "### ベルヌーイ分布\n",
    "$$\n",
    "P(x|u) = u^x(1-u)^{1-x}\n",
    "$$\n",
    "### 多項分布\n",
    "$$\n",
    "P(x|u,n) = _xC_nu^x(1-u)^{n-x}\n",
    "$$\n",
    "\n",
    "最尤法、交差クロスエントロピーなどとの関連\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c846349",
   "metadata": {},
   "source": [
    "## 第三章　情報理論\n",
    "要点まとめ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda85d26",
   "metadata": {},
   "source": [
    "### 自己情報量の説明　https://logics-of-blue.com/information-theory-basic/\n",
    "今持っている情報が多い（情報量）とき、新たに追加された情報の価値は事前に何も持っていない（情報量が少ない）ときよりも価値が高い。\n",
    "今持っている情報が多いイコール確率が低いことになるので、情報量iは確率の逆数としてあらわす。また加算として計算するために対数で表現する。\n",
    "自己情報量\n",
    "$$ i(x) = -log_2P(x) $$\n",
    "\n",
    "### 平均情報量\n",
    "$$\n",
    "H(x) = E(I(x)) = \\sum_{i+1}^{n} - P(x) \\times log(P(x))\n",
    "$$\n",
    "新たに情報が加わる際、すでに持っている情報量によってその重み（重量度）は異なる。\n",
    "新たに加わる情報の価値を平均情報量Ｈ(X)としてあらわす。<br>\n",
    "X:すでに1/5の情報（5個のうち１つになる）を知っているとき\n",
    "$$ \n",
    "平均情報量H(X) = \\sum_{i=1}^{5}-[P(xi)*log_2(xi)]  =  -\\frac{1}{5} \\times \\{log_2 \\frac{1}{5} + log_2 \\frac{1}{5}  +log_2 \\frac{1}{5} +log_2 \\frac{1}{5} +log_2 \\frac{1}{5} \\} = 2.32\n",
    "$$\n",
    "\n",
    "Y：すでに1/40の情報（40個のうち１つになる）を知っているとき\n",
    "$$\n",
    "平均情報量H(Y) = \\sum_{i=1}^{40}-\\{P(yi)*log_2(yi)\\}  =  -\\frac{1}{40} \\times \\{log_2 \\frac{1}{40} + log_2 \\frac{1}{40} + \\dots +log_2 \\frac{1}{40} +log_2 \\frac{1}{40} \\} = 5.32\n",
    "$$\n",
    "\n",
    "- XよりもYの時の方が新たに追加される時の方が「平均情報量」は高い\n",
    "- 知っている情報量が少ないときのほど、「平均情報量」が高い。\n",
    "- この「わからなさ」「不確実性」のことを「情報エントロピー」と呼ぶ。\n",
    "\n",
    "#### 情報エントロピー（シャノンエントロピー）　= 平均情報量\n",
    "\n",
    "### カルバック・ライブラー　ダイバージェンス / 相対エントロピー\n",
    "もともとこうだろうと思っていた確率分布P(x)に、違いがあるとわかった後の確率分布Q(x)の違いを知りたい。\n",
    "- それぞれの確率分布の情報量の差分の期待値　= カルバックライブラーダイバージェンス\n",
    "$$\n",
    "D_{KL}(P||Q) = \\sum Qlog_2 \\frac {Q}{P} = \\sum Q \\times \\{- log_2 P - (-log_2 Q)\\}\n",
    "$$\n",
    "すでに知っているときにその情報を追加されても情報量はゼロとなる。\n",
    "\n",
    "### 交差エントロピー　https://yaju3d.hatenablog.jp/entry/2018/11/30/225841　\n",
    "Qの平均情報量をPの情報量で表した（交差した）エントロピー\n",
    "$$\n",
    "Q(x) = \\sum_{i+1}^{n} - P(x) \\times log(Q(x))\n",
    "$$\n",
    "\n",
    "分類問題では交差エントロピー誤差を用いることで計算コストを下げ、また誤差が大きいときに減少幅を大きくできるメリットがある。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b60dbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
